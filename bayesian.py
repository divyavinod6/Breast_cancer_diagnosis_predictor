# -*- coding: utf-8 -*-
"""bayesian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUVAl2yVY4ZGRmwhlvkZZcmlzutzbwwZ
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("darkgrid")

data=pd.read_csv("/content/Breast_cancer_data.csv")

data.head(10)

data["diagnosis"].hist()

corr=data.iloc[:,:-1].corr(method="pearson")
cmap=sns.diverging_palette(250,354,80,60,center="dark",as_cmap=True)
sns.heatmap(corr,vmax=1,vmin=-0.5,cmap=cmap,square=True,linewidths=0.2)

data=data[["mean_radius","mean_texture","mean_smoothness","diagnosis"]]
data.head(10)

fig,axes=plt.subplots(1,3,figsize=(18,6),sharey=True)
sns.histplot(data,ax=axes[0],x="mean_radius",kde=True,color='r')
sns.histplot(data,ax=axes[1],x="mean_smoothness",kde=True,color='b')
sns.histplot(data,ax=axes[2],x="mean_texture",kde=True)

"""Calculate P(Y=y) for all possible y"""

def calculate_prior(df,Y):
  classes=sorted(list(df[Y].unique()))
  prior=[]
  for i in classes:
    prior.append(len(df[df[Y]==i])/len(df))
  return prior

"""To convert continues values into categorical values

Below function gives P(x|y) i.e conditional probability of x gives y
"""

def calculate_likelihood_gaussian(df,feat_name,feat_val,Y,label):
  feat=list(df.columns)
  df=df[df[Y]==label]
  mean,std=df[feat_name].mean(),df[feat_name].std()
  p_x_given_y=(1/(np.sqrt(2*np.pi)*std))*np.exp(-((feat_val-mean)**2/(2*std**2)))
  return p_x_given_y

"""P(X=x1|Y=y)P(X=x2|Y=y).. for all y and find the maximum"""

def naive_bayesian_gaussian(df,X,Y):
  #get features name
  features=list(df.columns)[:-1]
  #calculate prior
  prior=calculate_prior(df,Y)
  Y_pred=[]
  #loop over every data sample
  for x in X:
    # calculate likelihood 
    labels=sorted(list(df[Y].unique()))
    likelihood=[1]*len(labels)
    for j in range(len(labels)):
      for i in range(len(features)):
        likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])
    # calculate posterior probabilities (numerator only)
    post_prob=[1]*len(labels)
    for j in range(len(labels)):
      post_prob[j]=likelihood[j]*prior[j]
    Y_pred.append(np.argmax(post_prob))
  return np.array(Y_pred)

"""Testing 

"""

from sklearn.model_selection import train_test_split
train,test=train_test_split(data,test_size=0.2,random_state=41)

X_test=test.iloc[:,:-1].values
Y_test=test.iloc[:,-1].values
Y_pred=naive_bayesian_gaussian(train,X=X_test,Y='diagnosis')

from sklearn.metrics import confusion_matrix,f1_score
print(confusion_matrix(Y_test,Y_pred))
print(f1_score(Y_test,Y_pred))